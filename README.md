# AITest — 本地模型问答与评分助手

一个简单、好用的 Windows 桌面工具：管理题库（题目 + 标准答案），调用兼容 OpenAI 的聊天接口生成 AI 回答，并以人工或评分 API 给出 0–100 分与评语。非常适合快速评估本地/云端大模型在特定题库上的表现。

## 它能做什么
- 管理题库：以 `questions/*.json` 持久化题目、标准答案与评分记录
- 流式回答：实时显示模型输出，支持保存为该题的 AI 答案
- 附件随题提交：文本并入提示，图片以 Base64 Data URL 方式加入上下文
- 给分与评语：人工评分或调用可配置的评分 API，统一保存到题目文件
- 批量自动答题：一键为整套题生成非流式答案，带进度与日志
- 模型配置与测试：在界面配置 API 地址与密钥，测试连通并获取模型列表

## 适合谁用
- 需要用一套题库对不同模型做对比评估的同学
- 希望以最少操作把“提问—生成—评分—查看结果”串成闭环的用户

## 快速开始
1. 准备一个兼容 OpenAI `/v1/chat/completions` 的服务（如本地 `llama.cpp` HTTP server）
2. 打开“设置”，填入基础 URL、API Key、模型等信息并测试连接
3. 在主界面“添加问题”，写入题干与标准答案，必要时拖拽附件
4. 点击“提交问题”生成并保存 AI 回答；随后点击“打分”保存分数与评语
5. 在“评分列表”查看历史评分，打开“评语”对话框查看详情

## 目录与配置
- `questions/`：每道题一个 JSON（标题、题干、标准答案、附件、评分等）
- `files/`：附件文件所在目录（通过拖拽添加）
- `api_settings.json`：聊天接口配置（基础 URL、密钥、模型、采样参数、系统提示词）
- `score_api_settings.json`：评分接口列表（可维护多套评分端点与模型清单）

## 构建与运行
- Visual Studio 2022：打开解决方案并运行启动项目 `AITest`
- 命令行：`dotnet build` 与 `dotnet run`

## 注意事项
- 接口需兼容 `/v1/chat/completions` 并支持流式返回（SSE）
- 图片附件以数据 URL 传输，服务端需支持 `image_url` 输入
- 评分字段名在代码中为 `socre`（按现有拼写保持一致）
- `.gitignore` 已忽略常见构建产物：`bin/`、`obj/`、`.vs/` 等

欢迎把你的题库与评测流程交给 AITest，专注于模型对比与效果提升。

